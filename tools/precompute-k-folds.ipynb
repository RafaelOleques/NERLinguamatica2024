{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"CORPUS_NAME\"\n",
    "df = pd.read_json(f\"../corpora/{corpus_name}/{corpus_name}.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_classes_from_df(df):\n",
    "    entity_classes = set()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ner_tokens = row['ner_tokens']\n",
    "        for token in ner_tokens:\n",
    "            entity_classes.add(token.replace(\"B-\", \"\").replace(\"I-\", \"\"))\n",
    "\n",
    "    return list(entity_classes)\n",
    "\n",
    "# Extrair classes de entidades NER do DataFrame\n",
    "entity_classes = extract_entity_classes_from_df(df)\n",
    "\n",
    "print(\"Classes de Entidades NER encontradas:\")\n",
    "print(entity_classes)\n",
    "len(entity_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Convertendo todos os itens para minúsculas\n",
    "lowercase_labels_list = [item.lower() for item in entity_classes]\n",
    "\n",
    "# Contando as ocorrências de cada item\n",
    "counter = Counter(lowercase_labels_list)\n",
    "\n",
    "# Identificando os itens repetidos\n",
    "repeated_items = [item for item, count in counter.items() if count > 1]\n",
    "\n",
    "print(f\"Itens repetidos: {repeated_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_one_hot(bio_tags, entity_types):\n",
    "    \"\"\"\n",
    "    Converte uma lista de tags BIO para uma lista one-hot encoding.\n",
    "\n",
    "    Parameters:\n",
    "    bio_tags (list): Lista de tags BIO.\n",
    "    entity_types (list): Lista de tipos de entidades.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista one-hot encoding para as entidades.\n",
    "    \"\"\"\n",
    "    # Inicializa o vetor one-hot encoding com zeros\n",
    "    one_hot_vector = np.zeros(len(entity_types))\n",
    "\n",
    "    # Itera sobre as tags BIO\n",
    "    for tag in bio_tags:\n",
    "        # Se a tag não for 'O' (Outside)\n",
    "        if tag != 'O':\n",
    "            # Separa o prefixo (B ou I) do tipo da entidade\n",
    "            prefix, entity = tag.split('-')\n",
    "            # Marca a presença da entidade no vetor one-hot\n",
    "            if entity in entity_types:\n",
    "                one_hot_vector[entity_types.index(entity)] = 1\n",
    "\n",
    "    return one_hot_vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classes'] = df['ner_tokens'].apply(lambda x: bio_to_one_hot(x, entity_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['sentences'], inplace=True, keep=\"first\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sentences.values.tolist()\n",
    "y = df.classes.values.tolist()\n",
    "\n",
    "k_folds = 5\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def handout(X, y, df, n_splits, test_size, random_state=42):\n",
    "    df_train_aux = df.copy()\n",
    "    #Function to split data at train and validation\n",
    "    train_validation_folds = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    for train_index, validation_index in train_validation_folds.split(X, y):\n",
    "        #print(\"      TRAIN:\", train_index[:5], \"VALIDATION:\", validation_index[:5])\n",
    "        df_train = df_train_aux.copy()\n",
    "        df_test = df_train_aux.copy()\n",
    "        \n",
    "        #DF with train instances\n",
    "        df_train = df_train[df_train.index.isin(train_index)]\n",
    "\n",
    "        #DF with validation instances\n",
    "        df_test = df_test[df_test.index.isin(validation_index)]\n",
    "\n",
    "        #Saving splits\n",
    "        temp_dict = {\n",
    "            \"train\": df_train,\n",
    "            \"test\": df_test,\n",
    "        }\n",
    "\n",
    "        print(\"TRAIN:\", len(df_train), \"TEST:\", len(df_test),   \"TOTAL:\", len(df_train) + len(df_test))\n",
    "        print(\"==============\")\n",
    "        \n",
    "        return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cross_validation(X, y, df, n_splits, random_state=42, shuffle=True):\n",
    "    kfold = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "    #Save all the splits\n",
    "    train_validation_test = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kfold.split(X, y):\n",
    "        #DF auxiliar that will be used to generate train and test df\n",
    "        df_train_aux = df.copy()\n",
    "\n",
    "        #DF with test instances\n",
    "        df_test = df.copy()\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "        #Values to split train data at train and validation\n",
    "        X_train = [X[i] for i in train_index]\n",
    "        y_train = [y[i] for i in train_index]\n",
    "        \n",
    "        #DF to split train data at train and validation  \n",
    "        df_train_aux = df_train_aux[df_train_aux.index.isin(train_index)]\n",
    "        df_train_aux.index = [i for i in range(0, len(df_train_aux.index))]\n",
    "        \n",
    "        #DF with test instances\n",
    "        df_test = df_test[df_test.index.isin(test_index)]\n",
    "        \n",
    "        #Function to split data at train and validation\n",
    "        train_validation_folds = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=random_state)\n",
    "        \n",
    "        for train_index, validation_index in train_validation_folds.split(X_train, y_train):\n",
    "            #print(\"      TRAIN:\", train_index[:5], \"VALIDATION:\", validation_index[:5])\n",
    "            df_train = df_train_aux.copy()\n",
    "            df_validation = df_train_aux.copy()\n",
    "            \n",
    "            #DF with train instances\n",
    "            df_train = df_train[df_train.index.isin(train_index)]\n",
    "\n",
    "            #DF with validation instances\n",
    "            df_validation = df_validation[df_validation.index.isin(validation_index)]\n",
    "\n",
    "            #Saving splits\n",
    "            temp_dict = {\n",
    "                \"train\": df_train,\n",
    "                \"test\": df_test,\n",
    "                \"validation\": df_validation\n",
    "            }\n",
    "            train_validation_test.append(temp_dict)\n",
    "            print(\"TRAIN:\", len(df_train), \"TEST:\", len(df_test),  \"VALIDATION:\", len(df_validation),  \"TOTAL:\", len(df_train) + len(df_test) + len(df_validation))\n",
    "            print(\"==============\")\n",
    "    \n",
    "    return train_validation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação handout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handout_train_validation_test_list = handout(X, y, df, n_splits=2, test_size=0.2, random_state=random_state) #cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas2txt(df, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for _, line in df.iterrows():\n",
    "            for txt, tag in zip(line[\"tokens\"], line[\"ner_tokens\"]):\n",
    "                print(\"{} {}\".format(txt, tag), file=f_out)\n",
    "            print(file=f_out)\n",
    "\n",
    "    with open(path.replace(\"txt\", \"json\"), \"w\", encoding='utf-8') as outfile:\n",
    "        outfile.write(df.to_json(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas df\n",
    "df_train = handout_train_validation_test_list[\"train\"]\n",
    "df_train['classes'] = df_train['ner_tokens'].apply(lambda x: bio_to_one_hot(x, entity_classes))\n",
    "df_test = handout_train_validation_test_list[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.sentences.values.tolist()\n",
    "y = df_train.classes.values.tolist()\n",
    "\n",
    "k_folds = 5\n",
    "random_state=42\n",
    "\n",
    "handout_train_validation_list = handout(X, y, df_train, n_splits=2, test_size=0.1, random_state=random_state) #cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = handout_train_validation_list[\"train\"]\n",
    "df_validation = handout_train_validation_list[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'../corpora/{corpus_name}/labeled/1folds/fold0'\n",
    "os.makedirs(path)\n",
    "\n",
    "#Remove unnecessary columns\n",
    "df_train = df_train.drop(remove, axis=1)\n",
    "df_test = df_test.drop(remove, axis=1)\n",
    "df_validation = df_validation.drop(remove, axis=1)\n",
    "    \n",
    "pandas2txt(df_train, f'{path}/train.txt')\n",
    "pandas2txt(df_test, f'{path}/test.txt')\n",
    "pandas2txt(df_validation, f'{path}/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "Change the DataFrame to the handout training set if you want to perform cross-validation using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f\"../corpora/{corpus_name}/{corpus_name}.json\")\n",
    "df['classes'] = df['ner_tokens'].apply(lambda x: bio_to_one_hot(x, entity_classes))\n",
    "df.drop_duplicates(subset=['sentences'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.sentences.values.tolist()\n",
    "y = df.classes.values.tolist()\n",
    "\n",
    "k_folds = 5\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_test_list = cross_validation(X, y, df, k_folds, random_state=random_state) #cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_test_list[0][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(list(set(train_validation_test_list[0][\"validation\"][\"sentences\"]).intersection(set(train_validation_test_list[0][\"test\"][\"sentences\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'../corpora/{corpus_name}/labeled/{k_folds}folds/')\n",
    "\n",
    "for idx, df_dict in enumerate(train_validation_test_list):\n",
    "    #pandas df\n",
    "    df_train = df_dict[\"train\"]\n",
    "    df_test = df_dict[\"test\"]\n",
    "    df_validation = df_dict[\"validation\"]\n",
    "    \n",
    "    #Remove unnecessary columns\n",
    "    df_train = df_train.drop(remove, axis=1)\n",
    "    df_test = df_test.drop(remove, axis=1)\n",
    "    df_validation = df_validation.drop(remove, axis=1)\n",
    "    \n",
    "    #Creating folder\n",
    "    os.makedirs(f'../corpora/{corpus_name}/labeled/{k_folds}folds/fold{idx}')\n",
    "    \n",
    "    pandas2txt(df_train, f'../corpora/{corpus_name}/labeled/{k_folds}folds/fold{idx}/train.txt')\n",
    "    pandas2txt(df_validation, f'../corpora/{corpus_name}/labeled/{k_folds}folds/fold{idx}/dev.txt')\n",
    "    pandas2txt(df_test, f'../corpora/{corpus_name}/labeled/{k_folds}folds/fold{idx}/test.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
